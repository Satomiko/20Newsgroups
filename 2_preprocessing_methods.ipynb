{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPESDtjD5VseAQjuZrV+OG/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satomiko/20Newsgroups/blob/main/2_preprocessing_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOBCkmamZD0s",
        "outputId": "ca97c6cb-78dd-40ab-d652-ea4e99262904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove = (\"headers\", \"footers\", \"quotes\")\n",
        "news20group_train = fetch_20newsgroups(subset='train', remove = remove)"
      ],
      "metadata": {
        "id": "sgaFfxzDZRyy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert Bunch format to dataframe\n",
        "train_df = pd.DataFrame({'data': news20group_train.data, 'target': news20group_train.target})"
      ],
      "metadata": {
        "id": "CABLsA3xZYRp"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "train_df ['data'] = train_df ['data'] .apply(word_tokenize)\n",
        "\n",
        "train_df_copy = train_df  #creating a copy to test function"
      ],
      "metadata": {
        "id": "Lu6er-yGZiga"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#original\n",
        "print(train_df['data'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfv0k2bGZ2W7",
        "outputId": "f813232e-2b73-434b-c0a0-3e8700706463"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lower casing\n",
        "train_df['data'] = train_df['data'].apply(lambda tokens: [token.lower() for token in tokens])\n",
        "print(train_df['data'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4S_rYIQZ_o4",
        "outputId": "c6be4a33-bde1-4241-8dc6-8093ca244c90"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'i', 'saw', 'the', 'other', 'day', '.', 'it', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'it', 'was', 'called', 'a', 'bricklin', '.', 'the', 'doors', 'were', 'really', 'small', '.', 'in', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'this', 'is', 'all', 'i', 'know', '.', 'if', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop Word Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "train_df['data'] = train_df ['data'].apply (lambda tokens: [word for word in tokens if word not in stop_words])\n",
        "print(train_df['data'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUdbwtdjaGMs",
        "outputId": "a9cb39c5-3dd7-4be7-b013-4b72c6fa62f7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', '.', '2-door', 'sports', 'car', ',', 'looked', 'late', '60s/', 'early', '70s', '.', 'called', 'bricklin', '.', 'doors', 'really', 'small', '.', 'addition', ',', 'front', 'bumper', 'separate', 'rest', 'body', '.', 'know', '.', 'anyone', 'tellme', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'production', ',', 'car', 'made', ',', 'history', ',', 'whatever', 'info', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the word with only one-letter\n",
        "train_df['data'] = train_df['data'].apply(lambda tokens: [word for word in tokens if len(word) > 1])\n",
        "print(train_df['data'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xj2mlUsaMUX",
        "outputId": "d237b49a-78ad-4fb5-c0b4-081ddfa425f1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', '2-door', 'sports', 'car', 'looked', 'late', '60s/', 'early', '70s', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please', 'e-mail']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove special symbols and punctuation\n",
        "train_df['data'] = train_df['data'].apply(lambda tokens: [word for word in tokens if word.isalpha()])\n",
        "print(train_df['data'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCEyVa9iaTdP",
        "outputId": "a23e9f0d-8630-47ab-a3b9-c273d7443075"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sports', 'car', 'looked', 'late', 'early', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Stemming (according to the output reviewed, meaning of some words are compromized by this method. Stemming is eliminated from proprocessing method for this study)\n",
        "# stemmer = PorterStemmer()\n",
        "# train_df['data'] = train_df['data'].apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n",
        "\n",
        "# print(train_df['data'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUFbQznWaW0V",
        "outputId": "4719b93e-b786-451e-c67e-57ed2dea78c5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wonder', 'anyon', 'could', 'enlighten', 'car', 'saw', 'day', 'sport', 'car', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'realli', 'small', 'addit', 'front', 'bumper', 'separ', 'rest', 'bodi', 'know', 'anyon', 'tellm', 'model', 'name', 'engin', 'spec', 'year', 'product', 'car', 'made', 'histori', 'whatev', 'info', 'funki', 'look', 'car', 'plea']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatizing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "train_df['data'] = train_df['data'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n",
        "print(train_df['data'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHbIrcHebabJ",
        "outputId": "b28ef52a-f676-4063-8a84-edb924b47624"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sport', 'car', 'looked', 'late', 'early', 'called', 'bricklin', 'door', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a method to perform preprocessing by selected method\n",
        "\n",
        "def preprocess_text(df, column_name):\n",
        "    # Lowercasing\n",
        "    df[column_name] = df[column_name].apply(lambda tokens: [token.lower() for token in tokens])\n",
        "\n",
        "    # Stop Word Removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    df[column_name] = df[column_name].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
        "\n",
        "    # Removing one-letter words\n",
        "    df[column_name] = df[column_name].apply(lambda tokens: [word for word in tokens if len(word) > 1])\n",
        "\n",
        "    # Remove special symbols and punctuation\n",
        "    df[column_name] = df[column_name].apply(lambda tokens: [word for word in tokens if word.isalpha()])\n",
        "\n",
        "    # Lemmatizing\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    df[column_name] = df[column_name].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n"
      ],
      "metadata": {
        "id": "7TOk_Y1Qc5BF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing of the menthod\n",
        "preprocess_text(train_df_copy, 'data')\n",
        "print(train_df_copy['data'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrLzBnn2dkKi",
        "outputId": "9b26cc77-2a41-471f-8098-789e042666c5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sport', 'car', 'looked', 'late', 'early', 'called', 'bricklin', 'door', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please']\n"
          ]
        }
      ]
    }
  ]
}